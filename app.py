import os
import re
import streamlit as st
import numpy as np
import pandas as pd
from PIL import Image, ImageDraw, ImageFont
from io import BytesIO
import easyocr
from transformers import BlipProcessor, BlipForConditionalGeneration
import torch

# í˜ì´ì§€ ì„¤ì •: ì¤‘ì•™ ì •ë ¬
st.set_page_config(
    page_title="ì´ë¯¸ì§€ ê¸°ë°˜ PDP ìë™í™”",
    layout="centered"
)
st.markdown("""
<style>
  .main .block-container {
    max-width: 1000px !important;
    margin: 0 auto !important;
  }
</style>
""", unsafe_allow_html=True)

# -----------------------------
# ì§€ì—° ë¡œë”© ëª¨ë¸ ì´ˆê¸°í™”
# -----------------------------
@st.cache_resource
def load_models():
    reader = easyocr.Reader(['en'], gpu=False)
    processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
    blip_model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")
    return reader, processor, blip_model

# ì „ì—­ ëª¨ë¸ ë³€ìˆ˜
reader = None
processor = None
blip_model = None

# ìƒí’ˆ í‚¤ì›Œë“œ ì •ì˜
PRODUCT_KEYWORDS = {
    "refrigerator","fridge","tvmonitor","tv","oven",
    "microwave","washing machine","dishwasher",
    "laptop","cell phone","mobile","camera"
}

# -----------------------------
# UI: ì´ë¯¸ì§€ ì—…ë¡œë“œ ë° ì´ˆê¸°í™”
# -----------------------------
st.title("ì´ë¯¸ì§€ ê¸°ë°˜ PDP ìƒì„± ìë™í™” ì†”ë£¨ì…˜")
uploaded = st.file_uploader("ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”", type=["png","jpg","jpeg"])
if not uploaded:
    st.info("ì´ë¯¸ì§€ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”")
    st.stop()
img = Image.open(uploaded).convert("RGB")
source = getattr(uploaded, 'name', 'Uploaded Image')

# ìƒˆë¡œìš´ ì´ë¯¸ì§€ ì—…ë¡œë“œ ì‹œ ì„¸ì…˜ ì´ˆê¸°í™”
if "prev_source" not in st.session_state or st.session_state.prev_source != source:
    st.session_state.prev_source = source
    for key in ["candidates","choice","ocr_done","ocr_text","classified","recs","selected_component"]:
        st.session_state.pop(key, None)

# ì´ë¯¸ì§€ í‘œì‹œ
annotated = img.copy()
draw = ImageDraw.Draw(annotated)
font = ImageFont.load_default()
draw.text((10, 10), "1", fill="red", font=font)
st.image(annotated, use_container_width=True)
st.caption(f"Image Source: {source}")

# -----------------------------
# EasyOCR í…ìŠ¤íŠ¸ ì¶”ì¶œ
# -----------------------------
def extract_text_via_easyocr(pil_img: Image.Image) -> str:
    global reader
    arr = np.array(pil_img)
    lines = reader.readtext(arr, detail=0, paragraph=True)
    return "\n".join(lines)

# -----------------------------
# BLIP ìº¡ì…˜ ìƒì„±
# -----------------------------
def generate_blip_caption(pil_img: Image.Image) -> str:
    global processor, blip_model
    device = "cuda" if torch.cuda.is_available() else "cpu"
    inputs = processor(images=pil_img, return_tensors="pt").to(device)
    blip_model.to(device)
    out = blip_model.generate(**inputs, max_length=50)
    return processor.decode(out[0], skip_special_tokens=True)

# -----------------------------
# Alt Text í›„ë³´ ìƒì„±
# -----------------------------
def make_alt_candidates(pil_img: Image.Image):
    base = generate_blip_caption(pil_img)
    candidates = [base, f"{base} in a modern environment"]
    # LG ì œí’ˆ í‚¤ì›Œë“œ ì¶”ê°€ ì˜µì…˜
    if any(o.lower() in PRODUCT_KEYWORDS for o in []):
        candidates.append(f"LG product - {base}")
    # ì¤‘ë³µ ì œê±° ë° ìµœëŒ€ 3ê°œ
    unique = []
    for c in candidates:
        if c not in unique:
            unique.append(c)
        if len(unique) == 3:
            break
    return unique

# -----------------------------
# í…ìŠ¤íŠ¸ ë¶„ë¥˜ ë° ì»´í¬ë„ŒíŠ¸ ì¶”ì²œ
# -----------------------------
COMPONENT_DEFS = {
    "ST0001": {"name": "Hero banner", "has_image": True},
    "ST0002": {"name": "Tab Anchor", "has_image": False},
    "ST0003": {"name": "Title", "has_image": False},
    "ST0013": {"name": "Side Image", "has_image": True},
    "ST0014": {"name": "Layered Text", "has_image": True},
}
CTA_KEYWORDS = [
    "learn more","shop now","buy now","see more","read more",
    "click here","get started","try now","explore","discover","buy it now"
]

def classify_elements(lines):
    extracted, cleaned = [], []
    for L in lines:
        line = L
        for kw in CTA_KEYWORDS:
            if kw in line.lower():
                extracted.append(kw)
                line = re.sub(rf"\b{re.escape(kw)}\b", "", line, flags=re.IGNORECASE).strip()
        if line:
            cleaned.append(line)
    disclaimers = [l for l in cleaned if l.startswith("*")]
    body = [l for l in cleaned if not l.startswith("*")]
    ey = body.pop(0) if body and body[0].isupper() else ""
    hl = body.pop(0) if body else ""
    bc = "\n".join(body)
    return {"Eyebrow": ey, "Headline": hl, "Bodycopy": bc,
            "Disclaimer": "\n".join(disclaimers), "CTA": (extracted[0] if extracted else "")}

def recommend_components(classified, has_image=True):
    return [cid for cid, comp in COMPONENT_DEFS.items() if comp["has_image"] == has_image]

# -----------------------------
# Alt Text ìƒì„± (ì§€ì—° ë¡œë”©)
# -----------------------------
if st.button("ğŸ–¼ï¸ Alt Text ìƒì„±", key="alt_btn"):
    with st.spinner("ëª¨ë¸ ë¡œë”© ì¤‘â€¦ ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”"):
        r, p, m = load_models()
        reader, processor, blip_model = r, p, m
    st.session_state["candidates"] = make_alt_candidates(img)
    st.session_state.pop("choice", None)

if "candidates" in st.session_state:
    choice = st.radio("ìƒì„±ëœ Alt Text ì¤‘ ì„ íƒí•˜ì„¸ìš”:", st.session_state["candidates"], key="alt_choice")
    st.subheader("ğŸ–¼ï¸ Selected Alt Text")
    st.code(choice)

# -----------------------------
# OCR ì‹¤í–‰ (EasyOCR, ì§€ì—° ë¡œë”©)
# -----------------------------
if st.button("ğŸš€ OCR ì‹¤í–‰ (EasyOCR)", key="ocr_btn"):
    if reader is None:
        with st.spinner("ëª¨ë¸ ë¡œë”© ì¤‘â€¦ ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”"):
            r, p, m = load_models()
            reader, processor, blip_model = r, p, m
    txt = extract_text_via_easyocr(img)
    lines = txt.split("\n")
    st.session_state["ocr_done"] = True
    st.session_state["ocr_text"] = txt
    st.session_state["classified"] = classify_elements(lines)
    st.session_state["recs"] = recommend_components(st.session_state["classified"])

# -----------------------------
# ê²°ê³¼ ë Œë”ë§ ë° ì»´í¬ë„ŒíŠ¸ ì¶”ì²œ
# -----------------------------
if st.session_state.get("ocr_done"):
    st.subheader("ğŸ“‹ ì „ì²´ OCR ê²°ê³¼")
    st.text_area("", st.session_state["ocr_text"], height=200)
    st.subheader("ğŸ“‘ í…ìŠ¤íŠ¸ ìš”ì†Œ ë¶„ë¥˜ ê²°ê³¼")
    for k, v in st.session_state.get("classified", {}).items():
        st.markdown(f"**{k}:** {v or 'â€”'}")
    with st.expander("ğŸ§© ì»´í¬ë„ŒíŠ¸ ì¶”ì²œ"):
        recs = st.session_state.get("recs", [])
        if recs:
            sel = st.selectbox("ì¶”ì²œëœ ì»´í¬ë„ŒíŠ¸ ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒí•˜ì„¸ìš”:", recs,
                               format_func=lambda cid: f"{cid} â€“ {COMPONENT_DEFS[cid]['name']}")
            st.markdown(f"**Selected Component:** {sel} â€“ {COMPONENT_DEFS[sel]['name']}")
            st.session_state["selected_component"] = sel
        else:
            st.write("ì¶”ì²œí•  ì»´í¬ë„ŒíŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
    df = pd.DataFrame.from_dict(st.session_state.get("classified", {}), orient='index', columns=['Content'])
    csv = df.to_csv(encoding='utf-8-sig')
    st.download_button("ğŸ’¾ ì—‘ì…€ íŒŒì¼ ë‹¤ìš´ë¡œë“œ", data=csv, file_name="ocr_result.csv", mime="text/csv")

# -----------------------------
# PDP ìƒì„± ì „ì†¡ ì•ˆë‚´
# -----------------------------
if st.session_state.get("ocr_done") and st.button("ğŸ“¤ PDPìƒì„±í•˜ê¸° (WCMìœ¼ë¡œ ì „ì†¡í•˜ê¸°)", key="send"):
    st.caption("â€» í•´ë‹¹ ê¸°ëŠ¥ì€ ê¸°íš ë‹¨ê³„ì˜ êµ¬í˜„ì´ë©° ì‹¤ì œ ì ìš©ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
